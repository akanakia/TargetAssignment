\documentclass[11pt, onecolumn, compsoc, letterpaper]{article}


% Usual setup packages
\usepackage{times}
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage[margin=2.75cm]{geometry} % to change the page dimensions
\usepackage{graphicx} % General page control package
\usepackage[compact]{titlesec} % For compact title
\usepackage{listings} % For including source code with highlighting
\usepackage{subfiles} % For better multiple file per paper handling
\usepackage{hyperref} % For better hyper-link integration

% Packages for verbatim text blocks
\usepackage{alltt} % Package for including math in verbatim text
\usepackage{fancyvrb}

% Packages for math symbols and other mathey things
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{mathtools}

% Packages for including pseudo-code
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}

% Packages that handle lists
\usepackage{enumerate} % For reduced enumeration spacing
%\usepackage{enumitem} % For suppressing bullets
\usepackage{mdwlist} % Better control of lists

% Packages that handle tables, figures and other floats
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{titling}
\usepackage{float} % To make floats movable
\usepackage[font=scriptsize,labelfont=bf]{caption}
\usepackage[font=scriptsize,labelfont=bf]{subcaption}
\usepackage{hhline}
\usepackage[usenames,dvipsnames]{color}
\usepackage[table]{xcolor}

% Packages for drawing graphs, FSMs, etc.
\usepackage{pgf}
\usepackage{tikz}
\usepackage{tikz-qtree}
\usetikzlibrary{shapes,arrows,calc,fit,positioning,shapes.symbols,shapes.callouts,patterns,automata}

% clean up references
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}

% smaller tab space
\lstset{
	tabsize=4
}

%%% HEADERS & FOOTERS
\usepackage{titlepic}
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
%\usepackage{sectsty}
%\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
%\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
%\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
%\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
%\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

% Nice Little macro for adding a comment box. Includes incrementing comment numbers.
\newcounter{comcount}
\setcounter{comcount}{0}
\newcommand{\mycomment}[1]
{
\refstepcounter{comcount}
\smallskip\noindent\fbox{\parbox{\linewidth}{\emph{Comment \arabic{comcount}} : \small{#1}}} 
}

% Math commands
\newcommand{\vnorm}[1]{\left|\left|#1\right|\right|}
\newcommand{\tab}{\hspace*{2em}}
\DeclareMathOperator*{\argminop}{arg\,min\,}
\DeclareMathOperator*{\argmaxop}{arg\,max\,}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\newcommand{\argmin}[1]{\underset{#1}{\argminop}}
\newcommand{\argmax}[1]{\underset{#1}{\argmaxop}}
\newcommand{\D}[2]{\frac{d#1}{d#2}}
\newcommand{\PD}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\V}[1]{\mathbf{#1}}
\newcommand{\ubar}[1]{\underline{#1}}
\newcommand{\Ac}{\mathcal{A}} % (Bayesian) Action Profile
\newcommand{\St}{\mathcal{S}} % (Bayesian/Extensive) State List, Strategy set
\newcommand{\Pl}{\mathcal{N}} % (Extensive/Bayesian) Player List
\newcommand{\Hi}{\mathcal{H}} % (Extensive) Game History
\newcommand{\Pf}{\mathcal{P}} % (Extensive) Player Function
\newcommand{\Th}{\mathcal{Z}} % (Extensive) Terminal History
\newcommand{\Ta}{\mathcal{T}} % (Cooperative) Targets/Resources
\newcommand{\Co}{\mathcal{C}} % (Cooperative) Player assignment constraints
\newcommand{\We}{\mathcal{W}} % (All) Global Welfare Function

% Squeeze whitespace
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}
\setlength{\headsep}{0pt}
\setlength{\topskip}{0pt}
\setlength{\topmargin}{0pt}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}

\titlespacing{\section}{0pt}{*3}{*3}
\titlespacing{\subsection}{0pt}{*2}{*2}
\titlespacing{\subsubsection}{0pt}{*1}{*1}

\renewcommand{\arraystretch}{1.2}
\setlength{\droptitle}{-2cm}

\title{Performance Bounds on Distributed Target Assignment with Constraints}
\author{Anshul Kanakia}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Target assignment is a well studied problem in the fields of game theory and optimization where a number of agents are assigned to a number of targets for maximum system welfare. For example, given a set of search/surveillance zones and a swarm of UAVs, this problem describes how the UAVs should be delegated to search zones so as to maximize some global welfare for the system. An extension of this problem is where each search zone require a minimum number of agents to be assigned to it to receive any payoff whatsoever. Another natural extension arises when certain agents cannot be assigned to specific search areas for a number of reasons such as fuel constraints, obstacles, environmental hazards, etc. We look at both these extensions and frame the problem of target threshold assignment with agent constraints as a communal welfare game and discuss distributed solutions to this problem.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
The problem of target assignment (TA) is ubiquitous in different fields of research. In game theory, this problem is referred to as the Vehicle-Target Assignment problem. It is often called Task Allocation or Task Assignment in the field of robotics, specifically, swarm robotics and multi-agent systems. An equivalent problem is studied by ethologists to model division of labor in social insect colonies. In theoretical computer science there is a generalized formulation of target assignment called the Multiple Integer Knapsack Problem. While each of these formulations have provided unique perspectives and results, we believe an interdisciplinary approach is essential for truly leveraging the advantages broth forth via these different formulations of the common TA problem.

We begin by setting up the TA problem using concise and well understood nomenclature from game theory. This formulation is then used to set up an integer linear program (ILP) which optimizes a desired welfare function. The constraints for the ILP evolve naturally out of the game theory formulation as well. A discussion situating the TA problem within the well known P-NP computational complexity model from computer science follows. Here, we show that since the TA problem can be set up as an ILP, it follows that the yes-no decision version of TA is an NP-complete problem (through reduction to 3-SAT), while the TA optimization problem is NP-hard. Subsequently, we show that relaxing just one of the TA problem constrains results in an obvious centralized polynomial time greedy solution. Since the goal of this paper is to discuss bounds on the performance of \emph{distributed} approaches for solving TA, we compare the relaxed TA centralized algorithm with an analogous decentralized solution on a swarm of individually simple robots. We then re-introduce previously removed constraints and describe the complexities involved in designing a distributed solution of the TA problem. A theoretical comparison between centralized vs. decentralized approaches is once again afforded to us using Price of Anarchy (PoA) and Price of Stability (PoS) analysis tools from game theory. Finally, these theoretical results are backed up by simulated and real robot experiments.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Related Work}
Most recent work in distributed welfare games\cite{Marden2008, Marden2013} assumes that players are, in general, non-cooperative. While a similar formulation of the vehicle-target assignment problem has been studied by Arslan et al.\cite{Arslan2007}, the authors do not discuss the situation where targets have assignment thresholds as presented in this paper. As an aside, since target thresholds remove the submodularity property from welfare functions for this game, work on valid utility games by Vetta et al.\cite{Vetta2002} cannot be used in this circumstance and additional analysis must be done to predict bounds on Price of Anarchy and Stability (PoA/PoS).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Setup and Example}
The TA problem is formally defined as follows:
\begin{itemize}
	\item Agents/Players: $\Pl = \{n_1, n_2, \ldots, n_i, \ldots,n_{|\Pl|}\}$
	\item Targets: $\Ta = \{t_1, t_2, \ldots, t_j, \ldots,t_{|\Ta|}\}$
	\item Target Threshold: $K:\Ta \to \mathbb{N}$\\
	The number of agents required to successfully attempt task-$t_j$ is $= K(t_j)$, which is shortened to $k_j$ for brevity.
	
	\item Agent Constraints: $C:\Pl \to \hat{\Ta} \subseteq \Ta$\\
	The set of constraints for agent-$n_i$ ($= C(n_i)$) is the subset of targets that this agent can reach. It is shortened to $c_i$ for brevity.
	\item Agent Assignment Matrix: A $|\Pl| \times |\Ta|$ matrix of elements $x_{ij}$ that are either $0$ if agent-$i$ is not assigned to target-$j$ or $1$ if agent-$i$ is assigned to target-$j$.
	\begin{equation}\label{eq:X}
		X = \left(\begin{array}{ccc}
			x_{11} & \ldots & x_{1|\Ta|}\\
			\vdots & \ddots & \vdots\\
			x_{|\Pl|1} & \ldots & x_{|\Pl||\Ta|}
		\end{array}\right)
	\end{equation}
	\item Target Assignments: $A:\Ta \to \hat{\Pl} \subseteq \Pl$\\
	The set of agents assigned to target-$t_j$ is $= A(t_j)$, which is shortened to $a_j$ for brevity. From~\eqref{eq:X} we can define $|a_j| = \sum\limits_{i = 1}^{|\Pl|} x_{ij}$\\
	\textbf{Definition:} A target is considered ``successfully assigned'' when $|a_j| \geq k_j$, i.e. the number of player's assigned to it is greater than or equal to its threshold value.\\
	\textbf{Definition:} A target is considered ``perfectly assigned'' when $|a_j| = k_j$.
	\item Target specific welfare function,
\begin{align}
	W(t_j, |a_j|) & = \left\{
	\begin{array}{ll}
		w_j & |a_j| \geq k_j\\
		0 & o/w
	\end{array}\right.\label{eq:wf}
\end{align}
	\item Global welfare function,
\begin{align}
	\We = \sum\limits_{j = 1}^{|\Ta|} W(t_j, |a_j|)\label{eq:gwf}
\end{align}
\end{itemize}
Given this problem setup our goal is to generate an optimal assignment of agents to targets such that the global welfare is maximized while taking into account all agent constraints and target thresholds. Therefore the optimization problem looks something like, $\argmax{X} \We$ which is discussed in considerable detail in the next section.

The following example describes a cooperative game with target thresholds and player constraints as seen in Figure \ref{fig:ex1}.
\begin{itemize}
	\item Agent: $\Pl = \{1,2,3,4\}$
	\item Targets: $t \in \Ta = \{a, b, c\}$
	\item Target thresholds: $k_a = k_b = k_c = 2$
	\item Agent constraints: $c_1 = c_3 = \{a, b\}$ and $c_2 = c_4 = \{b, c\}$
	\item Target specific welfare function: $W(t_j, |a_j|) = 1$ (if $|a_j| \geq k_j$), $0$ otherwise.
	
\end{itemize}
\begin{figure}[!htb]
	\centering\includegraphics[width=5.5cm]{assets/ex1.png}
	\centering\caption{A cooperative game with 4 players and 3 targets. Gray pegs indicate the target's minimum threshold value while the arrows depict player assignment constraints.}\label{fig:ex1}
\end{figure}

Global welfare values for this example are listed in Table \ref{tab:mcgw}. Note that these values depend only on the number of successfully assigned targets. 
\begin{table}[!htb]
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{b}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & $2$ & $1$\\
	\cline{2-3}
	$b$ & $1$ & $1$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & $1$ & $2$\\
	\cline{2-3}
	$b$ & $1$ & $1$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,c$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & $1$ & $1$\\
	\cline{2-3}
	$b$ & $1$ & $1$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & $1$ & $1$\\
	\cline{2-3}
	$b$ & $1$ & $2$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,c$}
	\end{tabular}
	\centering\caption{Global welfare for the vehicle-target assignemt example scenario with target thresholds and player constraints.}\label{tab:mcgw}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Optimization using Integer Linear Programming (ILP)}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computational Complexity of the TA ILP problem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Distributed Approach to solving TA}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Selecting Agent Utility Functions}
We discuss the effects that choosing different utility functions has for this example scenario. We discuss two utility functions, marginal contribution and Shapley value utility, and apply log-linear learning to observe equilibria of the system under both utility design choices.

\subsubsection{Marginal Contribution (MC) Utility}
MC utility gives us two important and desirable properties for a good utility function, existence and efficiency of NE. NE existence is guaranteed because the chosen binary welfare function is anonymous and  depends only on the number of vehicles assigned to a target\cite{Monderer1996}. Given that the global objective function is just a counting function that counts the total number of successfully assigned targets, we can show that a maximum assignment is always an NE of the system as it results in maximum player payoffs (see proof in \textbf{Price of Stability} section). Player utility values using MC utility are listed in Table \ref{tab:mcUtil}.  

\begin{table}[!htb]
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{b}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & \textcolor{blue}{$1,1,1,1$} & $1,1,0,0$\\
	\cline{2-3}
	$b$ & $0,0,0,0$ & \textcolor{red}{$1,0,0,1$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $1,0,1,0$ & \textcolor{blue}{$1,1,1,1$}\\
	\cline{2-3}
	& \textcolor{red}{$1,1,0,0$} & $0,1,0,1$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,c$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0,0,0,0$ & \textcolor{red}{$0,0,1,1$}\\
	\cline{2-3}
	& \textcolor{red}{$0,0,0,0$} & $0,0,0,0$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& \textcolor{red}{$0,1,1,0$} & $0,1,0,1$\\
	\cline{2-3}
	& $0,0,0,0$ & \textcolor{blue}{$1,1,1,1$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,c$}
	\end{tabular}
	\centering\caption{Player payoffs for the example scenario using Marginal Contribution utility.}\label{tab:mcUtil}
\end{table}

Averaging over 1000 runs of log-linear learning for 1000 iterations each and using temperature $T = 0.15$, the following stationary distribution is observed (Table \ref{tab:mcsd}).

\begin{table}[!htb]
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{b}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & \textcolor{blue}{$0.343$} & $0.000$\\
	\cline{2-3}
	$b$ & $0.002$ & \textcolor{red}{$0.000$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0.001$ & \textcolor{blue}{$0.308$}\\
	\cline{2-3}
	& \textcolor{red}{$0.000$} & $0.000$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,c$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0.000$ & \textcolor{red}{$0.001$}\\
	\cline{2-3}
	& \textcolor{red}{$0.000$} & $0.000$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& \textcolor{red}{$0.000$} & $0.000$\\
	\cline{2-3}
	& $0.000$ & \textcolor{blue}{$0.345$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,c$}
	\end{tabular}
	\centering\caption{Stationary Distribution approximated using log-linear learning and MC utility.}\label{tab:mcsd}
\end{table}

As is evident from these results, applying log-linear learning using MC utility converges the system to perfect target assignment if a perfect assignment can be made. The NE in blue are all cases of perfect assignment while the equilibria in red are cases of successful assignment but do not maximize global welfare. We now look at what happens when Shapley value utility is used instead of MC for this game.

\subsubsection{Shapley Value (SV) Utility}
Using SV utility, each player's utility for selecting target, $t \in \Ta$, can be computed using the simplified equation,
\begin{align}
	U_i(t, a_{-i}) & = \left\{
		\begin{array}{ll}	
			1/|a|_t & |a|_t \geq k_t\\
			0 & |a|_t < k_t
		\end{array}\right.
\end{align}

Player utility values are listed in Table \ref{tab:svUtil}. Once again, running log-linear learning with the same parameters as used for marginal contribution utility we see the following stationary distribution (Table \ref{tab:svsd}).
\begin{table}[!htb]
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{b}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & \textcolor{blue}{$\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}$} & $\frac{1}{2},0\frac{1}{2},0$\\
	\cline{2-3}
	$b$ & $\frac{1}{3},\frac{1}{3},0,\frac{1}{3}$ & \textcolor{red}{$\frac{1}{2},0,0,\frac{1}{2}$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $\frac{1}{2},0,\frac{1}{2},0$ & \textcolor{blue}{$\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}$} \\
	\cline{2-3}
	& \textcolor{red}{$\frac{1}{2},\frac{1}{2},0,0$} & $0,\frac{1}{2},0\frac{1}{2}$ \\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,c$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0,\frac{1}{3},\frac{1}{3},\frac{1}{3}$ & \textcolor{red}{$0,0,\frac{1}{2},\frac{1}{2}$}\\
	\cline{2-3}
	& \textcolor{red}{$\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}$} & $\frac{1}{3},0,\frac{1}{3},\frac{1}{3}$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& \textcolor{red}{$0,\frac{1}{2},\frac{1}{2},0$} & $0,\frac{1}{2},0,\frac{1}{2}$\\
	\cline{2-3}
	& $\frac{1}{3},\frac{1}{3},\frac{1}{3},0$ & \textcolor{blue}{$\frac{1}{2},\frac{1}{2},\frac{1}{2},\frac{1}{2}$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,c$}
	\end{tabular}
	\centering\caption{Player payoffs using Shapley Value utility.}\label{tab:svUtil}
\end{table}
\begin{table}[!htb]
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{b}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	$a$ & \textcolor{blue}{$0.175$} & $0.004$\\
	\cline{2-3}
	$b$ & $0.055$ & \textcolor{red}{$0.005$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0.003$ & \textcolor{blue}{$0.159$}\\
	\cline{2-3}
	& \textcolor{red}{$0.004$} & $0.005$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$a,c$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& $0.057$ & \textcolor{red}{$0.002$}\\
	\cline{2-3}
	& \textcolor{red}{$0.266$} & $0.046$\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,b$}
	\end{tabular}~
	\centering\begin{tabular}{r|c|c|}
	\multicolumn{1}{c}{}
	& \multicolumn{1}{c}{$b$}
	& \multicolumn{1}{c}{$c$}\\
	\cline{2-3}
	& \textcolor{red}{$0.005$} & $0.004$\\
	\cline{2-3}
	& $0.056$ & \textcolor{blue}{$0.154$}\\
	\cline{2-3}
	\multicolumn{1}{c}{}
	& \multicolumn{2}{c}{$b,c$}
	\end{tabular}
	\centering\caption{Stationary Distribution approximated using log-linear learning and SV utility.}\label{tab:svsd}
\end{table}

In this case, the worst NE has the highest likelihood of being chosen by log-linear learning. The case when all the agents are assigned to the same target ($b, b, b, b$) results in both, minimum player utility as well as the lowest global welfare but is still an equilibrium of the game. While one can argue that the ``good'' equilibria---with equal global welfare and player payoffs---are collectively more likely to occur compared to the one bad equilibrium, this is still a $\approx 2:1 ((.175 + .159 + .154) / .266)$ ratio.

In this example case it is clearly a better idea to use MC over SV when picking an appropriate utility function for this system to do optimal target assignment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Performance Bounds: Centralized vs.~Distributed Approaches}
We can clearly see that the welfare function, $W(|a|_t)$, is not submodular.
\subsection{Price of Anarchy (PoA)}
PoA for this game is only defined under specific conditions for this vehicle-target assignment problem. The global welfare for the worst NE of the game can be $W_{NE}^{bad} = 0$, e.g. $|\Pl| = 3$, $|\Ta| = 3$, $c_i = \Ta$ $\forall i \in \Pl$, $k_t = 3$ $\forall t \in \Ta$. If every agent is assigned a unique target in this situation, then the system is at an equilibrium and no agent has any incentive to unilaterally deviate since they cannot improve their respective payoff or the global system welfare. In such a situation $W_{NE} = 0$ and thus $PoA = W_{OPT} / W_{NE}^{bad}$ would be undefined (or infinite).

In general, $W_{NE}^{bad} \not= 0$ when: for all actions in the valid action set, there must exist a target-$t$ such that it is either successfully assigned or is one agent away from being successfully assigned and an agent-$p$ exists that is not assigned to $t$ in the current action but has $t$ in it's target constraint set ($t \in c_p$, $a_p \not= t$).
\begin{align}
	\forall a \in 2^\Co, \exists t \in \Ta \text{ s.t. } |a|_t \geq (k_t - 1)\label{eq:poa}
\end{align}

If the property \eqref{eq:poa} holds for a given game then its PoA is bounded; $1 \leq PoA \leq W_{OPT}$. As more agents are ``added'' to the game (effectively creating a new game), the PoA goes down from $W_{OPT}$ to 1 when $|\Pl| \geq \sum\limits_{t \in \Ta} k_t$ (see Figure \ref{fig:poa}).

\begin{figure}[!htb]
	\centering\includegraphics[width=7.5cm]{assets/poa.png}
	\centering\caption{Bounds on PoA as $|\Pl|$ increases.}\label{fig:poa}
\end{figure}

\subsection{Price of Stability (PoS)}
PoS for this game is always 1. To prove this, assume we start with an action $a^{OPT}$ which results in optimal global welfare $W_{OPT}$. I will prove that this action $a^{OPT}$ must also be NE$^{best}$. If a player decides to unilaterally deviate from $a^{OPT}$ to $a'$ then $U_i(a'_i, a'_{-i}) > U_i(a^{OPT}_i, a^{OPT}_{-i})$. This means that the target $a'_i$ must not have been optimally assigned for action $a^{OPT}$. It is easy to see that a player's utility can only be increased if they move between successfully assigned targets or from some target (not necessarily successfully assigned) to a successfully assigned target. This results in one of two scenarios, either target $a^{OPT}_i$ was successfully assigned before the switch and is no longer successfully assigned in action $a'$ or both targets $a^{OPT}_i$ and $a'_i$ are successfully assigned in $a'$. 

If the first case is true then that means target $a'_i$ was successfully assigned in action $a'$ by the switch from player-$i$ but was not successfully assigned in action $a^{OPT}$ (this is the only way player-$i$ could receive a better payoff for deviating from $a^{OPT}$). This means the global welfare is reduced by 1 and increased by one when going from $a^{OPT}$ to $a'$ which means $a'$ is another optimal action and we can replace our starting action with this new one and continue this recursive argument.

If the second case is true then player-$i$ switch did nothing to the global welfare and we can once again replace $a^{OPT}$ with $a'$ and continue this argument.

Finally, if this player's deviation from $a^{OPT}$ to $a'$ resulted in a better payoff for any other reason, it means the new target chosen $a'_i$ was not successfully assigned in action $a^{OPT}$ but has been successfully assigned in action $a'$. This would imply $W(a') > W(a^{OPT})$ since action $a'$ now has one new successfully assigned target, $a'_i$. This would result in a contradiction since that would imply $a^{OPT}$ was never an optimum. Which means that no player-$i$ would deviate from $a^{OPT}$ for any reason, thereby making $a^{OPT}$ a NE. $a^{OPT}$ is also the best NE, $W(a^{OPT}) = W_{NE}^{best}$ by it's very definition and so PoS for this game is always 1.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
There are a number of further questions one can ask about the class of games discussed in this paper. My goal was mainly to show that game theory can be a viable alternative means to study cooperative scenarios with the system restrictions mentioned in this paper. While my bounds on PoA are not particularly satisfying, and rather obviously inferred just by thinking about such systems, perhaps a deeper analysis of the problem can provide exact values for PoA under special conditions\ldots Probably involving ratios between $|\Pl|$, $|\Ta|$ and $k_t$.

From the final section about finding the optimal global welfare for this class of games, I realized---to my amusement---that switching the problem around on it's head effectively invents a new method for solving the Knapsack Problem. All one has to do is define the knapsack problem as a cooperative game with target threshold values and performing log-linear learning on \emph{that} problem would give you an approximate solution to the original Knapsack problem!

As mentioned in the introduction, I find that this class of cooperative games definitely has a lot of potential for being a viable method for multi-agent, cooperative task design and analysis and should definitely be studied further.


% Bibliography
% \nocite{*} % Show all Bib-entries
\bibliographystyle{plainCustom}
\bibliography{refs}
\end{document}